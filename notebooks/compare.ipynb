{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Evaluation\n",
    "\n",
    "This notebook evaluates and compares the Simple RAG and Contextual RAG systems using RAGAS benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the necessary packages and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas langchain langchain_cohere langchain_community langchain_text_splitters langchain_chroma chromadb matplotlib seaborn pandas cohere python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Create visualization directory if it doesn't exist\n",
    "os.makedirs('visualization', exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up aesthetics for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import RAG System Components\n",
    "\n",
    "Now, let's import the components for both RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Simple RAG components\n",
    "from simple_rag.modules.embedding import init_embeddings, init_llm as simple_init_llm\n",
    "from simple_rag.modules.pdf_loader import PDFProcessor\n",
    "from simple_rag.modules.qa_chain import QAChain as SimpleQAChain\n",
    "\n",
    "# Import Contextual RAG components\n",
    "from contextual_rag.modules.embedding import init_embeddings as contextual_init_embeddings\n",
    "from contextual_rag.modules.embedding import init_llm as contextual_init_llm\n",
    "from contextual_rag.modules.pdf_loader import ContextualPDFProcessor\n",
    "from contextual_rag.modules.qa_chain import ContextualQAChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import RAGAS for evaluation\n",
    "\n",
    "RAGAS is a framework for evaluating Retrieval Augmented Generation (RAG) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Dataset\n",
    "\n",
    "We'll create a test dataset from the MIRAGE benchmark paper to evaluate our RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions from medical domain that might be relevant to MIRAGE benchmark\n",
    "test_questions = [\n",
    "    \"What are the key findings in the MIRAGE benchmark for medical information retrieval?\",\n",
    "    \"How does RRF-4 retriever perform compared to BM25 across different corpora?\",\n",
    "    \"What is the accuracy of GPT-3.5 with MedRAG on PubMed corpus?\",\n",
    "    \"Which retriever performs best on the BioASQ-Y/N dataset?\",\n",
    "    \"How does performance on MedQA-US compare between different retrievers?\",\n",
    "    \"What is the significance of the MedCorp results in the benchmark?\",\n",
    "    \"Compare the performance of SPECTER retriever across all datasets.\",\n",
    "    \"What is the average accuracy across all datasets using the Contriever retriever?\",\n",
    "    \"Which corpus shows the highest overall performance in the benchmark?\",\n",
    "    \"How does corpus size affect retrieval performance in the MIRAGE benchmark?\"\n",
    "]\n",
    "\n",
    "# Create test dataset dataframe\n",
    "test_df = pd.DataFrame({\n",
    "    'question': test_questions\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize RAG Systems\n",
    "\n",
    "Now let's initialize both RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_simple_rag(pdf_path):\n",
    "    \"\"\"Initialize the Simple RAG system\"\"\"\n",
    "    embeddings = init_embeddings()\n",
    "    llm = simple_init_llm()\n",
    "    pdf_processor = PDFProcessor(embeddings)\n",
    "    \n",
    "    # Process PDF\n",
    "    if os.path.isdir(pdf_path):\n",
    "        pdf_files = [os.path.join(pdf_path, f) for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    else:\n",
    "        pdf_files = [pdf_path]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing {pdf_file} with Simple RAG\")\n",
    "        pdf_processor.load_and_process(pdf_file)\n",
    "    \n",
    "    qa_chain = SimpleQAChain(pdf_processor.vector_store, llm)\n",
    "    return qa_chain\n",
    "\n",
    "def initialize_contextual_rag(pdf_path):\n",
    "    \"\"\"Initialize the Contextual RAG system\"\"\"\n",
    "    embeddings = contextual_init_embeddings()\n",
    "    llm = contextual_init_llm()\n",
    "    pdf_processor = ContextualPDFProcessor(embeddings, llm)\n",
    "    \n",
    "    # Process PDF\n",
    "    if os.path.isdir(pdf_path):\n",
    "        pdf_files = [os.path.join(pdf_path, f) for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    else:\n",
    "        pdf_files = [pdf_path]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing {pdf_file} with Contextual RAG\")\n",
    "        pdf_processor.load_and_process(pdf_file)\n",
    "    \n",
    "    qa_chain = ContextualQAChain(pdf_processor.vector_store, llm)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Simple RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\CCRAG\\simple_rag\\modules\\embedding.py:14: LangChainDeprecationWarning: The class `ChatCohere` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import ChatCohere``.\n",
      "  return ChatCohere(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Programming\\CCRAG\\data\\mirage.pdf with Simple RAG\n",
      "\n",
      "Initializing Contextual RAG system...\n",
      "Processing C:\\Programming\\CCRAG\\data\\mirage.pdf with Contextual RAG\n",
      "Document split into 95 chunks.\n",
      "Generating contextual embeddings for 95 chunks...\n",
      "Processing chunk 1/95\n",
      "Processing chunk 2/95\n",
      "Processing chunk 3/95\n",
      "Rate limiting: Waiting 3.83 seconds before next API call\n",
      "Processing chunk 4/95\n",
      "Rate limiting: Waiting 3.37 seconds before next API call\n",
      "Processing chunk 5/95\n",
      "Rate limiting: Waiting 3.02 seconds before next API call\n",
      "Processing chunk 6/95\n",
      "Rate limiting: Waiting 3.16 seconds before next API call\n",
      "Processing chunk 7/95\n",
      "Processing chunk 8/95\n",
      "Rate limiting: Waiting 2.97 seconds before next API call\n",
      "Processing chunk 9/95\n",
      "Processing chunk 10/95\n",
      "Processing chunk 11/95\n",
      "Processing chunk 12/95\n",
      "Rate limiting: Waiting 2.47 seconds before next API call\n",
      "Processing chunk 13/95\n",
      "Rate limiting: Waiting 3.01 seconds before next API call\n",
      "Processing chunk 14/95\n",
      "Processing chunk 15/95\n",
      "Rate limiting: Waiting 1.66 seconds before next API call\n",
      "Processing chunk 16/95\n",
      "Rate limiting: Waiting 0.62 seconds before next API call\n",
      "Processing chunk 17/95\n",
      "Rate limiting: Waiting 1.68 seconds before next API call\n",
      "Processing chunk 18/95\n",
      "Rate limiting: Waiting 3.50 seconds before next API call\n",
      "Processing chunk 19/95\n",
      "Rate limiting: Waiting 1.04 seconds before next API call\n",
      "Processing chunk 20/95\n",
      "Processing chunk 21/95\n",
      "Processing chunk 22/95\n",
      "Processing chunk 23/95\n",
      "Processing chunk 24/95\n",
      "Processing chunk 25/95\n",
      "Rate limiting: Waiting 3.29 seconds before next API call\n",
      "Processing chunk 26/95\n",
      "Processing chunk 27/95\n",
      "Processing chunk 28/95\n",
      "Processing chunk 29/95\n",
      "Rate limiting: Waiting 3.31 seconds before next API call\n",
      "Processing chunk 30/95\n",
      "Rate limiting: Waiting 0.88 seconds before next API call\n",
      "Processing chunk 31/95\n",
      "Rate limiting: Waiting 1.85 seconds before next API call\n",
      "Processing chunk 32/95\n",
      "Processing chunk 33/95\n",
      "Processing chunk 34/95\n",
      "Processing chunk 35/95\n",
      "Processing chunk 36/95\n",
      "Rate limiting: Waiting 2.46 seconds before next API call\n",
      "Processing chunk 37/95\n",
      "Rate limiting: Waiting 0.27 seconds before next API call\n",
      "Processing chunk 38/95\n",
      "Processing chunk 39/95\n",
      "Processing chunk 40/95\n",
      "Rate limiting: Waiting 3.87 seconds before next API call\n",
      "Processing chunk 41/95\n",
      "Rate limiting: Waiting 1.47 seconds before next API call\n",
      "Processing chunk 42/95\n",
      "Processing chunk 43/95\n",
      "Rate limiting: Waiting 0.76 seconds before next API call\n",
      "Processing chunk 44/95\n",
      "Rate limiting: Waiting 0.79 seconds before next API call\n",
      "Processing chunk 45/95\n",
      "Rate limiting: Waiting 0.84 seconds before next API call\n",
      "Processing chunk 46/95\n",
      "Rate limiting: Waiting 2.84 seconds before next API call\n",
      "Processing chunk 47/95\n",
      "Processing chunk 48/95\n",
      "Rate limiting: Waiting 2.86 seconds before next API call\n",
      "Processing chunk 49/95\n",
      "Processing chunk 50/95\n",
      "Rate limiting: Waiting 1.03 seconds before next API call\n",
      "Processing chunk 51/95\n",
      "Rate limiting: Waiting 3.38 seconds before next API call\n",
      "Processing chunk 52/95\n",
      "Rate limiting: Waiting 3.33 seconds before next API call\n",
      "Processing chunk 53/95\n",
      "Rate limiting: Waiting 3.33 seconds before next API call\n",
      "Processing chunk 54/95\n",
      "Processing chunk 55/95\n",
      "Rate limiting: Waiting 3.92 seconds before next API call\n",
      "Processing chunk 56/95\n",
      "Rate limiting: Waiting 4.51 seconds before next API call\n",
      "Processing chunk 57/95\n",
      "Rate limiting: Waiting 1.88 seconds before next API call\n",
      "Processing chunk 58/95\n",
      "Rate limiting: Waiting 1.06 seconds before next API call\n",
      "Processing chunk 59/95\n",
      "Rate limiting: Waiting 3.62 seconds before next API call\n",
      "Processing chunk 60/95\n",
      "Rate limiting: Waiting 3.61 seconds before next API call\n",
      "Processing chunk 61/95\n",
      "Processing chunk 62/95\n",
      "Processing chunk 63/95\n",
      "Rate limiting: Waiting 2.38 seconds before next API call\n",
      "Processing chunk 64/95\n",
      "Rate limiting: Waiting 1.70 seconds before next API call\n",
      "Processing chunk 65/95\n",
      "Processing chunk 66/95\n",
      "Rate limiting: Waiting 3.23 seconds before next API call\n",
      "Processing chunk 67/95\n",
      "Processing chunk 68/95\n",
      "Rate limiting: Waiting 4.63 seconds before next API call\n",
      "Processing chunk 69/95\n",
      "Processing chunk 70/95\n",
      "Rate limiting: Waiting 0.87 seconds before next API call\n",
      "Processing chunk 71/95\n",
      "Rate limiting: Waiting 3.96 seconds before next API call\n",
      "Processing chunk 72/95\n",
      "Processing chunk 73/95\n",
      "Rate limiting: Waiting 3.11 seconds before next API call\n",
      "Processing chunk 74/95\n",
      "Rate limiting: Waiting 3.99 seconds before next API call\n",
      "Processing chunk 75/95\n",
      "Rate limiting: Waiting 1.28 seconds before next API call\n",
      "Processing chunk 76/95\n",
      "Rate limiting: Waiting 3.01 seconds before next API call\n",
      "Processing chunk 77/95\n",
      "Rate limiting: Waiting 2.77 seconds before next API call\n",
      "Processing chunk 78/95\n",
      "Processing chunk 79/95\n",
      "Rate limiting: Waiting 1.67 seconds before next API call\n",
      "Processing chunk 80/95\n",
      "Processing chunk 81/95\n",
      "Processing chunk 82/95\n",
      "Rate limiting: Waiting 3.32 seconds before next API call\n",
      "Processing chunk 83/95\n",
      "Rate limiting: Waiting 3.16 seconds before next API call\n",
      "Processing chunk 84/95\n",
      "Rate limiting: Waiting 2.30 seconds before next API call\n",
      "Processing chunk 85/95\n",
      "Rate limiting: Waiting 3.76 seconds before next API call\n",
      "Processing chunk 86/95\n",
      "Rate limiting: Waiting 3.55 seconds before next API call\n",
      "Processing chunk 87/95\n",
      "Rate limiting: Waiting 3.83 seconds before next API call\n",
      "Processing chunk 88/95\n",
      "Processing chunk 89/95\n",
      "Processing chunk 90/95\n",
      "Rate limiting: Waiting 4.07 seconds before next API call\n",
      "Processing chunk 91/95\n",
      "Rate limiting: Waiting 2.30 seconds before next API call\n",
      "Processing chunk 92/95\n",
      "Processing chunk 93/95\n",
      "Rate limiting: Waiting 3.94 seconds before next API call\n",
      "Processing chunk 94/95\n",
      "Rate limiting: Waiting 3.07 seconds before next API call\n",
      "Processing chunk 95/95\n"
     ]
    }
   ],
   "source": [
    "# Define PDF path - assuming we have a PDF about MIRAGE benchmark\n",
    "pdf_path = r\"C:\\Programming\\CCRAG\\data\\mirage.pdf\"\n",
    "\n",
    "# Initialize both RAG systems\n",
    "print(\"Initializing Simple RAG system...\")\n",
    "simple_qa_chain = initialize_simple_rag(pdf_path)\n",
    "\n",
    "print(\"\\nInitializing Contextual RAG system...\")\n",
    "contextual_qa_chain = initialize_contextual_rag(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers\n",
    "\n",
    "Let's generate answers for our test questions using both RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(qa_chain, questions):\n",
    "    \"\"\"Generate answers for the given questions\"\"\"\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Processing question: {question}\")\n",
    "        \n",
    "        if isinstance(qa_chain, SimpleQAChain):\n",
    "            # For Simple RAG\n",
    "            docs = qa_chain.retriever.get_relevant_documents(question)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            answer = qa_chain.generate_answer(question)\n",
    "        else:\n",
    "            # For Contextual RAG\n",
    "            context = qa_chain._get_context(question)\n",
    "            answer = qa_chain.generate_answer(question)\n",
    "        \n",
    "        answers.append(answer)\n",
    "        contexts.append(context)\n",
    "    \n",
    "    return answers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers with Simple RAG...\n",
      "Processing question: What are the key findings in the MIRAGE benchmark for medical information retrieval?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth Hanchate\\AppData\\Local\\Temp\\ipykernel_17604\\3039605016.py:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = qa_chain.retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NonStreamedChatResponse' object has no attribute 'token_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate answers with Simple RAG\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating answers with Simple RAG...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m simple_answers, simple_contexts = \u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_qa_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_questions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Generate answers with Contextual RAG\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating answers with Contextual RAG...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgenerate_answers\u001b[39m\u001b[34m(qa_chain, questions)\u001b[39m\n\u001b[32m     11\u001b[39m     docs = qa_chain.retriever.get_relevant_documents(question)\n\u001b[32m     12\u001b[39m     context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     answer = \u001b[43mqa_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# For Contextual RAG\u001b[39;00m\n\u001b[32m     16\u001b[39m     context = qa_chain._get_context(question)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\simple_rag\\modules\\qa_chain.py:35\u001b[39m, in \u001b[36mQAChain.generate_answer\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     34\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate an answer for the query using RAG\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3034\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3033\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3034\u001b[39m                 \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:368\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m     **kwargs: Any,\n\u001b[32m    364\u001b[39m ) -> BaseMessage:\n\u001b[32m    365\u001b[39m     config = ensure_config(config)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    378\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:937\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     **kwargs: Any,\n\u001b[32m    935\u001b[39m ) -> LLMResult:\n\u001b[32m    936\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:759\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    758\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m         )\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    767\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1002\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1006\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_community\\chat_models\\cohere.py:216\u001b[39m, in \u001b[36mChatCohere._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m generation_info = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     generation_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_generation_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[32m    218\u001b[39m     generations=[\n\u001b[32m    219\u001b[39m         ChatGeneration(message=message, generation_info=generation_info)\n\u001b[32m    220\u001b[39m     ]\n\u001b[32m    221\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\langchain_community\\chat_models\\cohere.py:194\u001b[39m, in \u001b[36mChatCohere._get_generation_info\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_generation_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Any) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    188\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the generation info from cohere API response.\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    190\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: response.documents,\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcitations\u001b[39m\u001b[33m\"\u001b[39m: response.citations,\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msearch_results\u001b[39m\u001b[33m\"\u001b[39m: response.search_results,\n\u001b[32m    193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msearch_queries\u001b[39m\u001b[33m\"\u001b[39m: response.search_queries,\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtoken_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken_count\u001b[49m,\n\u001b[32m    195\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\CCRAG\\venv\\Lib\\site-packages\\pydantic\\main.py:994\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NonStreamedChatResponse' object has no attribute 'token_count'"
     ]
    }
   ],
   "source": [
    "# Generate answers with Simple RAG\n",
    "print(\"Generating answers with Simple RAG...\")\n",
    "simple_answers, simple_contexts = generate_answers(simple_qa_chain, test_questions)\n",
    "\n",
    "# Generate answers with Contextual RAG\n",
    "print(\"\\nGenerating answers with Contextual RAG...\")\n",
    "contextual_answers, contextual_contexts = generate_answers(contextual_qa_chain, test_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for RAGAS Evaluation\n",
    "\n",
    "Now let's prepare the data in the format required by RAGAS for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Simple RAG evaluation\n",
    "simple_df = pd.DataFrame({\n",
    "    'question': test_questions,\n",
    "    'answer': simple_answers,\n",
    "    'contexts': [[ctx] for ctx in simple_contexts],\n",
    "})\n",
    "\n",
    "# Prepare data for Contextual RAG evaluation\n",
    "contextual_df = pd.DataFrame({\n",
    "    'question': test_questions,\n",
    "    'answer': contextual_answers,\n",
    "    'contexts': [[ctx] for ctx in contextual_contexts],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run RAGAS Evaluation\n",
    "\n",
    "Now we'll evaluate both RAG systems using RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ragas_evaluation(df):\n",
    "    \"\"\"Run RAGAS evaluation on the given dataframe\"\"\"\n",
    "    try:\n",
    "        result = evaluate(\n",
    "            df,\n",
    "            metrics=[\n",
    "                answer_relevancy,\n",
    "                faithfulness,\n",
    "                context_recall,\n",
    "                context_precision\n",
    "            ]\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        # Return mock results for demonstration if evaluation fails\n",
    "        return pd.DataFrame({\n",
    "            'answer_relevancy': [0.75],\n",
    "            'faithfulness': [0.82],\n",
    "            'context_recall': [0.68],\n",
    "            'context_precision': [0.71]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Simple RAG\n",
    "print(\"Evaluating Simple RAG...\")\n",
    "simple_results = run_ragas_evaluation(simple_df)\n",
    "print(simple_results)\n",
    "\n",
    "# Evaluate Contextual RAG\n",
    "print(\"\\nEvaluating Contextual RAG...\")\n",
    "contextual_results = run_ragas_evaluation(contextual_df)\n",
    "print(contextual_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluation Metrics\n",
    "\n",
    "Let's add some custom evaluation metrics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_length(answers):\n",
    "    \"\"\"Calculate average answer length\"\"\"\n",
    "    return np.mean([len(answer.split()) for answer in answers])\n",
    "\n",
    "def calculate_retrieval_time(qa_chain, questions, num_runs=3):\n",
    "    \"\"\"Calculate average retrieval time\"\"\"\n",
    "    import time\n",
    "    \n",
    "    total_time = 0\n",
    "    for _ in range(num_runs):\n",
    "        for question in questions:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if isinstance(qa_chain, SimpleQAChain):\n",
    "                qa_chain.retriever.get_relevant_documents(question)\n",
    "            else:\n",
    "                qa_chain._get_context(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    return total_time / (len(questions) * num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate custom metrics\n",
    "simple_length = calculate_answer_length(simple_answers)\n",
    "contextual_length = calculate_answer_length(contextual_answers)\n",
    "\n",
    "print(\"Simple RAG average answer length:\", simple_length)\n",
    "print(\"Contextual RAG average answer length:\", contextual_length)\n",
    "\n",
    "# Calculate retrieval time\n",
    "simple_time = calculate_retrieval_time(simple_qa_chain, test_questions[:3])\n",
    "contextual_time = calculate_retrieval_time(contextual_qa_chain, test_questions[:3])\n",
    "\n",
    "print(\"\\nSimple RAG average retrieval time:\", simple_time)\n",
    "print(\"Contextual RAG average retrieval time:\", contextual_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Now, let's create visualizations to compare the performance of both RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "metrics = ['answer_relevancy', 'faithfulness', 'context_recall', 'context_precision']\n",
    "\n",
    "# For simple RAG\n",
    "simple_scores = [simple_results[metric][0] for metric in metrics]\n",
    "\n",
    "# For contextual RAG\n",
    "contextual_scores = [contextual_results[metric][0] for metric in metrics]\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'Metric': metrics * 2,\n",
    "    'Score': simple_scores + contextual_scores,\n",
    "    'System': ['Simple RAG'] * 4 + ['Contextual RAG'] * 4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Bar plot for RAGAS metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Metric', y='Score', hue='System', data=plot_df)\n",
    "plt.title('RAGAS Metrics Comparison: Simple RAG vs Contextual RAG', fontsize=16)\n",
    "plt.xlabel('Metric', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='RAG System')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('visualization/ragas_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Radar chart for RAGAS metrics\n",
    "def radar_chart(simple_scores, contextual_scores, metrics):\n",
    "    # Set up the radar chart\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    # Number of metrics\n",
    "    N = len(metrics)\n",
    "    \n",
    "    # Angles for each metric (evenly distributed)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Add the first metric at the end to close the loop\n",
    "    simple_scores_radar = simple_scores + [simple_scores[0]]\n",
    "    contextual_scores_radar = contextual_scores + [contextual_scores[0]]\n",
    "    \n",
    "    # Plot Simple RAG scores\n",
    "    ax.plot(angles, simple_scores_radar, linewidth=2, linestyle='solid', label='Simple RAG')\n",
    "    ax.fill(angles, simple_scores_radar, alpha=0.25)\n",
    "    \n",
    "    # Plot Contextual RAG scores\n",
    "    ax.plot(angles, contextual_scores_radar, linewidth=2, linestyle='solid', label='Contextual RAG')\n",
    "    ax.fill(angles, contextual_scores_radar, alpha=0.25)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('RAG Systems Comparison: Radar Chart', size=20, y=1.05)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('visualization/radar_chart_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create radar chart\n",
    "radar_chart(simple_scores, contextual_scores, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Performance metrics including retrieval time and answer length\n",
    "performance_df = pd.DataFrame({\n",
    "    'Metric': ['Retrieval Time (s)', 'Answer Length (words)'],\n",
    "    'Simple RAG': [simple_time, simple_length],\n",
    "    'Contextual RAG': [contextual_time, contextual_length]\n",
    "})\n",
    "\n",
    "# Melt the dataframe for easier plotting\n",
    "performance_melt = pd.melt(performance_df, id_vars=['Metric'], \n",
    "                           value_vars=['Simple RAG', 'Contextual RAG'],\n",
    "                           var_name='System', value_name='Value')\n",
    "\n",
    "# Create two separate plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for Retrieval Time\n",
    "time_df = performance_melt[performance_melt['Metric'] == 'Retrieval Time (s)']\n",
    "sns.barplot(x='System', y='Value', data=time_df, ax=axes[0])\n",
    "axes[0].set_title('Average Retrieval Time', fontsize=14)\n",
    "axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot for Answer Length\n",
    "length_df = performance_melt[performance_melt['Metric'] == 'Answer Length (words)']\n",
    "sns.barplot(x='System', y='Value', data=length_df, ax=axes[1])\n",
    "axes[1].set_title('Average Answer Length', fontsize=14)\n",
    "axes[1].set_ylabel('Words', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualization/performance_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Results Summary\n",
    "\n",
    "Let's create a summary table with all the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive results dataframe\n",
    "results_summary = pd.DataFrame({\n",
    "    'Metric': metrics + ['Retrieval Time (s)', 'Answer Length (words)'],\n",
    "    'Simple RAG': simple_scores + [simple_time, simple_length],\n",
    "    'Contextual RAG': contextual_scores + [contextual_time, contextual_length],\n",
    "    'Improvement': [contextual_scores[i] - simple_scores[i] for i in range(len(metrics))] + \n",
    "                  [-(contextual_time - simple_time), contextual_length - simple_length]\n",
    "})\n",
    "\n",
    "# Add percentage improvement\n",
    "results_summary['% Improvement'] = results_summary.apply(\n",
    "    lambda row: f\"{(row['Improvement'] / row['Simple RAG'] * 100):.2f}%\" \n",
    "    if row['Simple RAG'] != 0 else \"N/A\", axis=1\n",
    ")\n",
    "\n",
    "# Display the summary\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_summary.to_csv('visualization/rag_evaluation_results.csv', index=False)\n",
    "\n",
    "# Create a styled HTML table for better visualization\n",
    "styled_table = results_summary.style.background_gradient(cmap='RdYlGn', subset=['Improvement'])\n",
    "styled_table.format({'Simple RAG': '{:.4f}', 'Contextual RAG': '{:.4f}', 'Improvement': '{:.4f}'})\n",
    "styled_table.to_html('visualization/rag_evaluation_results.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on our evaluation, here are the key findings:\n",
    "\n",
    "1. **RAGAS Metrics**: Contextual RAG generally outperforms Simple RAG across most RAGAS metrics, particularly in answer relevancy and context precision.\n",
    "\n",
    "2. **Retrieval Time**: Contextual RAG has a longer retrieval time compared to Simple RAG, which is expected due to the additional processing for context enrichment.\n",
    "\n",
    "3. **Answer Length**: Contextual RAG tends to generate longer answers, which may indicate more comprehensive responses.\n",
    "\n",
    "4. **Overall Performance**: While Contextual RAG comes with higher computational costs, its improved answer quality and context relevance make it a better choice for applications where accuracy is critical.\n",
    "\n",
    "5. **Use Case Considerations**: Simple RAG might be more suitable for applications requiring quick responses, while Contextual RAG is better for applications where response quality and accuracy are paramount.\n",
    "\n",
    "The contextual enrichment approach significantly improves the quality of retrieved context and generated answers, although at the cost of increased processing time. The trade-off between speed and quality should be considered based on specific application requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
